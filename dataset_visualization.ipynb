{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from huanzhi_utils import load_file, write_list_of_dicts_to_file\n",
    "import os\n",
    "from const import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "models = [model for model in MODELS if model in os.listdir('/Users/raymondtsao/Documents/BFCL_Visualization_Test/result_score/score')]\n",
    "print(len(models))\n",
    "\n",
    "result_standardized = []\n",
    "import json\n",
    "\n",
    "def standardize_step_response(input_step_response):\n",
    "    step_response = {}\n",
    "    step_response[\"assistant_response\"] = input_step_response[0]\n",
    "    step_response[\"handler_response\"]  = input_step_response[1]\n",
    "    step_response[\"tool_response\"] = input_step_response[2:]\n",
    "    step_response[\"num_tools\"] = len(\n",
    "        step_response[\"handler_response\"].get(\"model_response_decoded\", [])\n",
    "    )\n",
    "    return step_response\n",
    "\n",
    "def standardize_turn_response(result):\n",
    "    input_turn_response, end_of_turn_state = result    \n",
    "    turn_response = {}\n",
    "    turn_response[\"end_of_turn_state\"] = end_of_turn_state\n",
    "    turn_response[\"turn_eval_message\"] = input_turn_response[\"begin_of_turn_query\"]\n",
    "    keys = list(input_turn_response.keys())\n",
    "    keys.remove(\"begin_of_turn_query\") \n",
    "    keys = sorted(keys, key=lambda x: int(x.split(\"_\")[-1])) \n",
    "    step_responses = []\n",
    "    for idx, key in enumerate(keys):\n",
    "        assert key.endswith(f\"_{idx}\")\n",
    "        step_responses.append(input_turn_response[key])\n",
    "    \n",
    "    turn_response[\"step_responses\"] = [standardize_step_response(step_response) for step_response in step_responses]\n",
    "    turn_response[\"num_steps\"] = len(step_responses) - 1\n",
    "    return turn_response\n",
    "\n",
    "def standardize_inference_response(result):\n",
    "    inference_response = {}\n",
    "    inference_response[\"id\"] = result[\"id\"]\n",
    "    inference_log = result[\"inference_log\"]\n",
    "    inference_response[\"initial_api_state\"] = inference_log[0] # a list of states\n",
    "    num_turns = (len(inference_log) - 1)//2 # the first one is the initial state\n",
    "    inference_response[\"num_turns\"] = num_turns\n",
    "    inference_log = inference_log[1:] # remove the initial state\n",
    "    inference_response[\"turn_responses\"] = [\n",
    "        standardize_turn_response(inference_log[i : i + 2])\n",
    "        for i in range(0, len(inference_log), 2)\n",
    "    ]\n",
    "    return inference_response\n",
    "\n",
    "def standardize_inference_response_error(result):\n",
    "    inference_response = {}\n",
    "    inference_response[\"id\"] = result[\"id\"]\n",
    "    inference_response[\"num_turns\"] = 1\n",
    "    inference_response[\"turn_responses\"] = [\n",
    "        'Error during inference: Request timed out.'\n",
    "    ]\n",
    "    return inference_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_dataframe(model, category):\n",
    "    result_file_path = f'/Users/raymondtsao/Documents/BFCL_Visualization_Test/result_score/result/{model}/BFCL_v3_multi_turn_{category}_result.json'\n",
    "    result_list = load_file(result_file_path)\n",
    "    result_standardized = []\n",
    "    for result in result_list:\n",
    "        try:\n",
    "            result_standardized.append(standardize_inference_response(result))\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    return pd.DataFrame(result_standardized)\n",
    "\n",
    "def get_score_dataframe(model, category):\n",
    "    def extract_error_types(error_dict):\n",
    "        error_types = []\n",
    "        for key, value in error_dict.items():\n",
    "            if key == \"error_type\":\n",
    "                error_types.append(value)\n",
    "            elif isinstance(value, dict) and \"error_type\" in value:\n",
    "                error_types.append(value[\"error_type\"])\n",
    "        return error_types\n",
    "    try:\n",
    "        score_file_path = f'/Users/raymondtsao/Documents/BFCL_Visualization_Test/result_score/score/{model}/BFCL_v3_multi_turn_{category}_score.json'\n",
    "        score_list = load_file(score_file_path)\n",
    "        data = []\n",
    "        score_list_metrics = score_list[1:]  # First entry in the score json file is an accuracy summary and that is not useful for now\n",
    "        for item in score_list_metrics:\n",
    "            data.append({\n",
    "                \"id\": item[\"id\"],\n",
    "                \"valid\": item[\"valid\"],\n",
    "                \"error_type\": extract_error_types(item[\"error\"])\n",
    "            })\n",
    "        score_df = pd.DataFrame(data)\n",
    "        return score_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing score file {score_file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "errored_model_cateogry = []\n",
    "for model in models:\n",
    "    for category in ['base', 'long_context', 'miss_func', 'miss_param']:\n",
    "        result_df = get_result_dataframe(model, category)\n",
    "        score_df = get_score_dataframe(model, category)\n",
    "        if result_df is None or score_df is None:\n",
    "            errored_model_cateogry.append((model, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15677966101694915\n"
     ]
    }
   ],
   "source": [
    "print(len(errored_model_cateogry) / (4*len(MODELS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23387096774193547\n"
     ]
    }
   ],
   "source": [
    "print(len(errored_model_cateogry) / (4*len(models)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
