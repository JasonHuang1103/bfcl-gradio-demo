{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from huanzhi_utils import load_file, write_list_of_dicts_to_file\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Qwen_Qwen2.5-1.5B-Instruct', 'deepseek-ai_DeepSeek-Coder-V2-Instruct-0724', 'palmyra-x-004', 'claude-3-opus-20240229', 'gemini-1.5-pro-002-FC', 'open-mixtral-8x22b-FC', 'gemini-1.5-flash-002', 'openbmb_MiniCPM3-4B', 'gemini-1.0-pro-002', 'ibm-granite_granite-20b-functioncalling', 'Salesforce_xLAM-8x22b-r', 'Nexusflow-Raven-v2', 'mistral-medium-2312', 'Qwen_Qwen2-7B-Instruct', 'Salesforce_xLAM-7b-r', 'Qwen_Qwen2.5-72B-Instruct', 'firefunction-v1-FC', 'deepseek-ai_DeepSeek-Coder-V2-Lite-Instruct', 'meta-llama_Llama-3.1-70B-Instruct', 'mistral-small-2402-FC', 'gemini-1.0-pro-002-FC', 'databricks-dbrx-instruct', 'open-mixtral-8x7b', 'claude-3-opus-20240229-FC', 'gpt-3.5-turbo-0125-FC', 'gemini-1.5-flash-002-FC', 'NousResearch_Hermes-2-Pro-Mistral-7B', 'NousResearch_Hermes-2-Pro-Llama-3-8B', 'google_gemma-2-27b-it', 'MadeAgents_Hammer2.0-0.5b', 'Team-ACE_ToolACE-8B', 'open-mistral-nemo-2407', 'mistral-small-2402', 'gpt-4o-2024-08-06-FC', 'google_gemma-2-2b-it', 'gpt-4o-mini-2024-07-18-FC', 'meta-llama_Llama-3.2-1B-Instruct', 'claude-3-5-haiku-20241022', 'MadeAgents_Hammer2.0-1.5b', 'gemini-1.5-pro-001', 'meta-llama_Llama-3.1-8B-Instruct-FC', 'command-r-plus', 'openbmb_MiniCPM3-4B-FC', 'mistral-large-2407-FC', 'claude-3-5-sonnet-20241022', 'gemini-1.5-flash-001', 'open-mixtral-8x22b', 'gpt-4o-2024-08-06', 'claude-3-haiku-20240307', 'meta-llama_Meta-Llama-3-70B-Instruct', 'MadeAgents_Hammer2.0-7b', 'Salesforce_xLAM-7b-fc-r', 'Salesforce_xLAM-8x7b-r', 'meta-llama_Meta-Llama-3-8B-Instruct', 'command-r-plus-FC', 'meta-llama_Llama-3.1-8B-Instruct', 'nova-micro-v1.0', 'gemini-1.5-flash-001-FC', 'THUDM_glm-4-9b-chat', 'open-mistral-nemo-2407-FC', 'claude-3-haiku-20240307-FC', 'nova-lite-v1.0', 'google_gemma-2-9b-it', 'meetkai_functionary-medium-v3.1-FC', 'Salesforce_xLAM-1b-fc-r', 'o1-mini-2024-09-12', 'BitAgent_GoGoAgent', 'firefunction-v2-FC', 'Qwen_Qwen2.5-7B-Instruct', 'mistral-large-2407', 'o1-preview-2024-09-12', 'gpt-4-turbo-2024-04-09-FC', 'meta-llama_Llama-3.1-70B-Instruct-FC', 'gemini-1.5-pro-002', 'gpt-3.5-turbo-0125', 'gemini-1.5-pro-001-FC', 'meta-llama_Llama-3.2-3B-Instruct', 'meetkai_functionary-small-v3.1-FC', 'gpt-4o-mini-2024-07-18', 'claude-3-5-sonnet-20241022-FC', 'Qwen_Qwen2-1.5B-Instruct', 'nova-pro-v1.0', 'gpt-4-turbo-2024-04-09']\n"
     ]
    }
   ],
   "source": [
    "result_file_path = '/Users/raymondtsao/Documents/BFCL_Visualization_Test/result_score/result'\n",
    "models = [model for model in os.listdir(result_file_path) if model != '.DS_Store']\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_standardized = []\n",
    "import json\n",
    "\n",
    "def standardize_step_response(input_step_response):\n",
    "    step_response = {}\n",
    "    step_response[\"assistant_response\"] = input_step_response[0]\n",
    "    step_response[\"handler_response\"]  = input_step_response[1]\n",
    "    step_response[\"tool_response\"] = input_step_response[2:]\n",
    "    step_response[\"num_tools\"] = len(\n",
    "        step_response[\"handler_response\"].get(\"model_response_decoded\", [])\n",
    "    )\n",
    "    return step_response\n",
    "\n",
    "def standardize_turn_response(result):\n",
    "    input_turn_response, end_of_turn_state = result    \n",
    "    turn_response = {}\n",
    "    turn_response[\"end_of_turn_state\"] = end_of_turn_state\n",
    "    turn_response[\"turn_eval_message\"] = input_turn_response[\"begin_of_turn_query\"]\n",
    "    keys = list(input_turn_response.keys())\n",
    "    keys.remove(\"begin_of_turn_query\") \n",
    "    keys = sorted(keys, key=lambda x: int(x.split(\"_\")[-1])) \n",
    "    step_responses = []\n",
    "    for idx, key in enumerate(keys):\n",
    "        assert key.endswith(f\"_{idx}\")\n",
    "        step_responses.append(input_turn_response[key])\n",
    "    \n",
    "    turn_response[\"step_responses\"] = [standardize_step_response(step_response) for step_response in step_responses]\n",
    "    turn_response[\"num_steps\"] = len(step_responses) - 1\n",
    "    return turn_response\n",
    "\n",
    "def standardize_inference_response(result):\n",
    "    inference_response = {}\n",
    "    inference_response[\"id\"] = result[\"id\"]\n",
    "    inference_log = result[\"inference_log\"]\n",
    "    inference_response[\"initial_api_state\"] = inference_log[0] # a list of states\n",
    "    num_turns = (len(inference_log) - 1)//2 # the first one is the initial state\n",
    "    inference_response[\"num_turns\"] = num_turns\n",
    "    inference_log = inference_log[1:] # remove the initial state\n",
    "    inference_response[\"turn_responses\"] = [\n",
    "        standardize_turn_response(inference_log[i : i + 2])\n",
    "        for i in range(0, len(inference_log), 2)\n",
    "    ]\n",
    "    return inference_response\n",
    "\n",
    "def standardize_inference_response_error(result):\n",
    "    inference_response = {}\n",
    "    inference_response[\"id\"] = result[\"id\"]\n",
    "    inference_response[\"num_turns\"] = 1\n",
    "    inference_response[\"turn_responses\"] = [\n",
    "        'Error during inference: Request timed out.'\n",
    "    ]\n",
    "    return inference_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "errored_model = []\n",
    "for model in models:\n",
    "    result_file_path = f'/Users/raymondtsao/Documents/BFCL_Visualization_Test/result_score/result/{model}/BFCL_v3_multi_turn_base_result.json'\n",
    "    result_list = load_file(result_file_path)\n",
    "    for result in result_list:\n",
    "        try:\n",
    "            result_standardized.append(standardize_inference_response(result))\n",
    "        except Exception as e:\n",
    "            errored_model.append(model)\n",
    "            try:\n",
    "                if result['result'] == 'Error during inference: Request timed out.':\n",
    "                    result_standardized.append(standardize_inference_response_error(result))\n",
    "            except Exception as e:\n",
    "                print(f\"Error standardizing {model}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Qwen_Qwen2.5-1.5B-Instruct', 'claude-3-opus-20240229', 'gemini-1.5-pro-002-FC', 'gemini-1.5-flash-002', 'openbmb_MiniCPM3-4B', 'gemini-1.0-pro-002', 'Salesforce_xLAM-8x22b-r', 'Nexusflow-Raven-v2', 'Qwen_Qwen2-7B-Instruct', 'Salesforce_xLAM-7b-r', 'Qwen_Qwen2.5-72B-Instruct', 'firefunction-v1-FC', 'deepseek-ai_DeepSeek-Coder-V2-Lite-Instruct', 'meta-llama_Llama-3.1-70B-Instruct', 'mistral-small-2402-FC', 'databricks-dbrx-instruct', 'claude-3-opus-20240229-FC', 'gpt-3.5-turbo-0125-FC', 'NousResearch_Hermes-2-Pro-Mistral-7B', 'NousResearch_Hermes-2-Pro-Llama-3-8B', 'MadeAgents_Hammer2.0-0.5b', 'Team-ACE_ToolACE-8B', 'open-mistral-nemo-2407', 'mistral-small-2402', 'gpt-4o-2024-08-06-FC', 'google_gemma-2-2b-it', 'gpt-4o-mini-2024-07-18-FC', 'meta-llama_Llama-3.2-1B-Instruct', 'MadeAgents_Hammer2.0-1.5b', 'gemini-1.5-pro-001', 'meta-llama_Llama-3.1-8B-Instruct-FC', 'command-r-plus', 'openbmb_MiniCPM3-4B-FC', 'mistral-large-2407-FC', 'claude-3-5-sonnet-20241022', 'gemini-1.5-flash-001', 'open-mixtral-8x22b', 'gpt-4o-2024-08-06', 'claude-3-haiku-20240307', 'MadeAgents_Hammer2.0-7b', 'Salesforce_xLAM-8x7b-r', 'meta-llama_Meta-Llama-3-8B-Instruct', 'meta-llama_Llama-3.1-8B-Instruct', 'THUDM_glm-4-9b-chat', 'open-mistral-nemo-2407-FC', 'claude-3-haiku-20240307-FC', 'google_gemma-2-9b-it', 'Salesforce_xLAM-1b-fc-r', 'Qwen_Qwen2.5-7B-Instruct', 'mistral-large-2407', 'gpt-4-turbo-2024-04-09-FC', 'meta-llama_Llama-3.1-70B-Instruct-FC', 'gpt-3.5-turbo-0125', 'gemini-1.5-pro-001-FC', 'meta-llama_Llama-3.2-3B-Instruct', 'gpt-4o-mini-2024-07-18', 'claude-3-5-sonnet-20241022-FC', 'Qwen_Qwen2-1.5B-Instruct', 'gpt-4-turbo-2024-04-09']\n"
     ]
    }
   ],
   "source": [
    "models_not_errored = [model for model in models if model not in errored_model]\n",
    "print(models_not_errored)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_path = f'/Users/raymondtsao/Documents/BFCL_Visualization_Test/result_score/result/{errored_model[1]}/BFCL_v3_multi_turn_base_result.json'\n",
    "result_list = load_file(result_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error standardizing 0: 'inference_log'\n",
      "Error standardizing 2: 'inference_log'\n",
      "Error standardizing 21: 'inference_log'\n",
      "Error standardizing 39: 'inference_log'\n",
      "Error standardizing 40: 'inference_log'\n",
      "Error standardizing 52: 'inference_log'\n",
      "Error standardizing 56: 'inference_log'\n",
      "Error standardizing 61: 'inference_log'\n",
      "Error standardizing 68: 'inference_log'\n",
      "Error standardizing 69: 'inference_log'\n",
      "Error standardizing 70: 'inference_log'\n",
      "Error standardizing 71: 'inference_log'\n",
      "Error standardizing 72: 'inference_log'\n",
      "Error standardizing 73: 'inference_log'\n",
      "Error standardizing 74: 'inference_log'\n",
      "Error standardizing 75: 'inference_log'\n",
      "Error standardizing 76: 'inference_log'\n",
      "Error standardizing 77: 'inference_log'\n",
      "Error standardizing 78: 'inference_log'\n",
      "Error standardizing 79: 'inference_log'\n",
      "Error standardizing 80: 'inference_log'\n",
      "Error standardizing 81: 'inference_log'\n",
      "Error standardizing 82: 'inference_log'\n",
      "Error standardizing 83: 'inference_log'\n",
      "Error standardizing 84: 'inference_log'\n",
      "Error standardizing 85: 'inference_log'\n",
      "Error standardizing 86: 'inference_log'\n",
      "Error standardizing 87: 'inference_log'\n",
      "Error standardizing 88: 'inference_log'\n",
      "Error standardizing 89: 'inference_log'\n",
      "Error standardizing 90: 'inference_log'\n",
      "Error standardizing 91: 'inference_log'\n",
      "Error standardizing 92: 'inference_log'\n",
      "Error standardizing 93: 'inference_log'\n",
      "Error standardizing 94: 'inference_log'\n",
      "Error standardizing 95: 'inference_log'\n",
      "Error standardizing 96: 'inference_log'\n",
      "Error standardizing 97: 'inference_log'\n",
      "Error standardizing 98: 'inference_log'\n",
      "Error standardizing 99: 'inference_log'\n",
      "Error standardizing 100: 'inference_log'\n",
      "Error standardizing 101: 'inference_log'\n",
      "Error standardizing 102: 'inference_log'\n",
      "Error standardizing 103: 'inference_log'\n",
      "Error standardizing 104: 'inference_log'\n",
      "Error standardizing 105: 'inference_log'\n",
      "Error standardizing 106: 'inference_log'\n",
      "Error standardizing 107: 'inference_log'\n",
      "Error standardizing 108: 'inference_log'\n",
      "Error standardizing 109: 'inference_log'\n",
      "Error standardizing 110: 'inference_log'\n",
      "Error standardizing 111: 'inference_log'\n",
      "Error standardizing 112: 'inference_log'\n",
      "Error standardizing 113: 'inference_log'\n",
      "Error standardizing 114: 'inference_log'\n",
      "Error standardizing 115: 'inference_log'\n",
      "Error standardizing 116: 'inference_log'\n",
      "Error standardizing 117: 'inference_log'\n",
      "Error standardizing 118: 'inference_log'\n",
      "Error standardizing 119: 'inference_log'\n",
      "Error standardizing 120: 'inference_log'\n",
      "Error standardizing 121: 'inference_log'\n",
      "Error standardizing 122: 'inference_log'\n",
      "Error standardizing 123: 'inference_log'\n",
      "Error standardizing 124: 'inference_log'\n",
      "Error standardizing 125: 'inference_log'\n",
      "Error standardizing 126: 'inference_log'\n",
      "Error standardizing 127: 'inference_log'\n",
      "Error standardizing 128: 'inference_log'\n",
      "Error standardizing 129: 'inference_log'\n",
      "Error standardizing 130: 'inference_log'\n",
      "Error standardizing 131: 'inference_log'\n",
      "Error standardizing 132: 'inference_log'\n",
      "Error standardizing 133: 'inference_log'\n",
      "Error standardizing 134: 'inference_log'\n",
      "Error standardizing 135: 'inference_log'\n",
      "Error standardizing 136: 'inference_log'\n",
      "Error standardizing 137: 'inference_log'\n",
      "Error standardizing 138: 'inference_log'\n",
      "Error standardizing 139: 'inference_log'\n",
      "Error standardizing 140: 'inference_log'\n",
      "Error standardizing 141: 'inference_log'\n",
      "Error standardizing 142: 'inference_log'\n",
      "Error standardizing 143: 'inference_log'\n",
      "Error standardizing 144: 'inference_log'\n",
      "Error standardizing 145: 'inference_log'\n",
      "Error standardizing 146: 'inference_log'\n",
      "Error standardizing 147: 'inference_log'\n",
      "Error standardizing 148: 'inference_log'\n",
      "Error standardizing 149: 'inference_log'\n",
      "Error standardizing 150: 'inference_log'\n",
      "Error standardizing 151: 'inference_log'\n",
      "Error standardizing 152: 'inference_log'\n",
      "Error standardizing 153: 'inference_log'\n",
      "Error standardizing 154: 'inference_log'\n",
      "Error standardizing 155: 'inference_log'\n",
      "Error standardizing 156: 'inference_log'\n",
      "Error standardizing 157: 'inference_log'\n",
      "Error standardizing 158: 'inference_log'\n",
      "Error standardizing 159: 'inference_log'\n",
      "Error standardizing 160: 'inference_log'\n",
      "Error standardizing 161: 'inference_log'\n",
      "Error standardizing 162: 'inference_log'\n",
      "Error standardizing 163: 'inference_log'\n",
      "Error standardizing 164: 'inference_log'\n",
      "Error standardizing 165: 'inference_log'\n",
      "Error standardizing 166: 'inference_log'\n",
      "Error standardizing 167: 'inference_log'\n",
      "Error standardizing 168: 'inference_log'\n",
      "Error standardizing 169: 'inference_log'\n",
      "Error standardizing 170: 'inference_log'\n",
      "Error standardizing 171: 'inference_log'\n",
      "Error standardizing 172: 'inference_log'\n",
      "Error standardizing 173: 'inference_log'\n",
      "Error standardizing 174: 'inference_log'\n",
      "Error standardizing 175: 'inference_log'\n",
      "Error standardizing 176: 'inference_log'\n",
      "Error standardizing 177: 'inference_log'\n",
      "Error standardizing 178: 'inference_log'\n",
      "Error standardizing 179: 'inference_log'\n",
      "Error standardizing 180: 'inference_log'\n",
      "Error standardizing 181: 'inference_log'\n",
      "Error standardizing 182: 'inference_log'\n",
      "Error standardizing 183: 'inference_log'\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(result_list):\n",
    "    try:\n",
    "        s = standardize_inference_response(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error standardizing {i}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'multi_turn_base_0',\n",
       " 'result': 'Error during inference: Request timed out.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model error rate: 0.2891566265060241\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model error rate: {len(errored_model)/len(models)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
